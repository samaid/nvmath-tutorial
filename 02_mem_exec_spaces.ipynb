{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0ab10b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#192015; color:#7fa637; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "\n",
    "![nvmath-python](_assets/nvmath_head_panel@0.25x.png)\n",
    "\n",
    "<p style=\"font-size:0.85em; margin-top:8px;\">\n",
    "Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES<br>\n",
    "SPDX-License-Identifier: BSD-3-Clause\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89dd057",
   "metadata": {},
   "source": [
    "# Getting started with nvmath-python: memory and execution spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcffa0",
   "metadata": {},
   "source": [
    "In this tutorial we provide basic 101 about **nvmath-python**, how it fits in and plays with an existing scientific computing ecosystem in Python and what makes it a useful addition for this ecosystem. This notebook touches upon memory and execution space concepts foundations in the nvmath-python operation\n",
    "\n",
    "<div class=\"alert alert-box alert-info\">\n",
    "    To use this notebook you will need a computer equipped with NVIDIA GPU as well as an environment with properly installed Python libraries and (optionally) CUDA Toolkit. Please refer to the nvmath-python documentation for getting familiar with <a href=\"https://docs.nvidia.com/cuda/nvmath-python/0.2.1/installation.html#install-nvmath-python\">installation options</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dd080",
   "metadata": {},
   "source": [
    "## Memory and execution spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc193e7a",
   "metadata": {},
   "source": [
    "Flexibility of choosing a corresponding array library (or multiple libraries!) to interoperate with applies to both GPU-only libraries (e.g. CuPy), CPU-libraries libraries (e.g. NumPy), and CPU-GPU libraries (.e.g. PyTorch). It is possible because nvmath-python is backed by both GPU libraries (such as cuBLAS or cuFFT) and CPU libraries (NVPL for NVIDIA Grace CPUs and Intel MKL for x86 hosts). This allows easy code migration between CPU and GPU as well as an implementation of complex hybrid workflows that combine both CPU and GPU execution.\n",
    "\n",
    "The *memory space* is memory dedicated for storing input data and results. It is tied to a specific device (or a host) and is allocated and released by means of respective device/host API call. The *execution space* is where the data is actually processed. Memory and execution spaces are not necessarily the same. This is important to remember because data transfer between memory spaces often incurs non-negligible costs. These costs may be high not only in the case of data movement between a host CPU and a GPU device, but also between two GPU devices.\n",
    "\n",
    "Let's take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import nvmath\n",
    "\n",
    "m, n, k = 8000, 2000, 4000\n",
    "a_cpu = np.random.randn(m, k).astype(np.float32)\n",
    "b_cpu = np.random.randn(k, n).astype(np.float32)\n",
    "\n",
    "a_gpu = cp.random.randn(m, k, dtype=cp.float32)\n",
    "b_gpu = cp.random.randn(k, n, dtype=cp.float32)\n",
    "\n",
    "d_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "d_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "type(d_cpu)  # numpy.ndarray\n",
    "type(d_gpu)  # cupy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a304266",
   "metadata": {},
   "source": [
    "We will use again the helper for benchmarking the codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d45e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ce61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a_gpu, b_gpu),\n",
    "    F_name=\"Matmul with GPU inputs\",\n",
    "    F_alternative=lambda: nvmath.linalg.advanced.matmul(a_cpu, b_cpu),\n",
    "    F_alternative_name=\"Matmul with CPU inputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b886ac",
   "metadata": {},
   "source": [
    "The difference is noticeable but where does the cost come from? Indeed, `nvmath.linalg.advanced.matmul` belongs to a category of *specialized APIs*. In contrast to *generic APIs* such as `nvmath.fft.fft`, specialized ones serve very specific needs, which comes at a cost of generality. Specifically, `nvmath.linalg.advanced.matmul` supports GPU execution space only. When `nvmath.linalg.advanced.matmul` receives CPU tensor inputs, it inherently copies them into its *execution space*, then performs operation, and then copies result back to the original *memory space*.\n",
    "\n",
    "The next example illustrates what is happening under-the-hood of nvmath-python through library's logging mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to show info messages from nvmath and its internals\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)  # ensure logging is enabled\n",
    "\n",
    "# Run matmul with GPU inputs (execution space == GPU)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* GPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "d_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "print(\"d_gpu type:\", type(d_gpu))\n",
    "\n",
    "# Run matmul with CPU inputs (this will cause nvmath to copy to execution space internally)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* CPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "d_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "print(\"d_cpu type:\", type(d_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb42f12",
   "metadata": {},
   "source": [
    "In the case of GPU inputs the `= SPECIFICATION PHASE =` section reports:\n",
    "\n",
    "`The input operands' memory space is cuda, and the execution space is on device 0.`\n",
    "\n",
    "while in the case of CPU inputs the report is different:\n",
    "\n",
    "`The input operands' memory space is cpu, and the execution space is on device 0.`\n",
    "\n",
    "Such as significant overhead cannot be ignored, for sure. The nvmath-python's logging mechanism is a great tool to understand potential costs and refactor the code accordingly to minimize the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c145f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Exercise 2. Estimate CPU-GPU data transfer overhead</h3>\n",
    "We see non-negligible performance difference between data residing in CPU memory space vs. GPU memory space in the above example. Given that execution space is always GPU, estimate data transfer cost. Implement a dedicated benchmark for a cross-check.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fec27",
   "metadata": {},
   "source": [
    "\n",
    "Next, let us illustrate the data flow in the case of the library's **fast Fourier transforms (FFT)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "e_cpu = (np.random.randn(N) + 1j * np.random.randn(N)).astype(np.complex64)\n",
    "e_gpu = cp.array(e_cpu)  # move NumPy data to GPU as CuPy array (complex64)\n",
    "\n",
    "# compute FFT with nvmath (for CPU inputs nvmath may copy to execution space internally)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* CPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "r_cpu = nvmath.fft.fft(e_cpu)\n",
    "print(\"r_cpu type:\", type(r_cpu), getattr(r_cpu, \"dtype\", None), getattr(r_cpu, \"shape\", None))\n",
    "\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* GPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "r_gpu = nvmath.fft.fft(e_gpu)\n",
    "print(\"r_gpu type:\", type(r_gpu), getattr(r_gpu, \"dtype\", None), getattr(r_gpu, \"shape\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185f550",
   "metadata": {},
   "source": [
    "Take a note that when input operand is a CPU operand then the library choses execution space to be CPU, thanks to the fact that FFT belongs to *generic APIs* providing consistent behavior between CPU and GPU. In the case of GPU inputs the library selects GPU as an execution space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c0f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Takeaways</h3>\n",
    "\n",
    "- Memory space (where data is stored) and execution space (where computation happens) may differ, leading to data transfer costs.\n",
    "- Some specialized APIs, e.g. `nvmath.linalg.advanced.matmul`, only support GPU execution, automatically transferring CPU data to GPU with associated overhead.\n",
    "- Generic APIs like `nvmath.fft.fft` adapt to input location: CPU inputs execute on CPU, GPU inputs execute on GPU.\n",
    "- Use nvmath-python's logging mechanism to understand internal operations and identify potential bottlenecks.\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
