{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04262e8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#192015; color:#7fa637; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "\n",
    "![nvmath-python](_assets/nvmath_head_panel@0.25x.png)\n",
    "\n",
    "<p style=\"font-size:0.85em; margin-top:8px;\">\n",
    "Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES<br>\n",
    "SPDX-License-Identifier: BSD-3-Clause\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0dba77",
   "metadata": {},
   "source": [
    "# Getting started with nvmath-python: stateful APIs, autotuning\n",
    "\n",
    "In this tutorial we provide basic 101 about **nvmath-python**, how it fits in and plays with an existing scientific computing ecosystem in Python and what makes it a useful addition for this ecosystem. \n",
    "<div class=\"alert alert-box alert-info\">\n",
    "    To use this notebook you will need a computer equipped with NVIDIA GPU as well as an environment with properly installed Python libraries and (optionally) CUDA Toolkit. Please refer to the nvmath-python documentation for getting familiar with <a href=\"https://docs.nvidia.com/cuda/nvmath-python/0.2.1/installation.html#install-nvmath-python\">installation options</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd41a2",
   "metadata": {},
   "source": [
    "This section aims at introduction to nvmath-python stateful APIs and autotuning. \n",
    "\n",
    "But first, let's borrow the benchmarking helper function we created in the previous section to continue performance experiments with stateful APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33311026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a01a",
   "metadata": {},
   "source": [
    "Let us consider a scenario which is typical in neural networks. We will perform in a batch a series of matrix-matrix multiplications combined with bias and **ReLU** as an *activation*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2812fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "\n",
    "m, n, k = 124, 10, 15\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "d = cp.empty((m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(m, dtype=cp.float32)\n",
    "\n",
    "d = nvmath.linalg.advanced.matmul(a, b, epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias})\n",
    "\n",
    "print(\"d type =\", type(d))\n",
    "print(\"d shape =\", d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b3808",
   "metadata": {},
   "source": [
    "## Stateful and stateless APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3c606",
   "metadata": {},
   "source": [
    "What we used so far is *stateless* (or *function-form*) API for `matmul` in nvmath-python. This is a convenience API allowing to perform a desired operation as a single function call and get the result. Under the hood, however, a lot of machinery has been going before actual computation took place. Let us illustrate that by using nvmath-python's logging capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate what happens under the hood using nvmath-python's logging capabilities\n",
    "import logging\n",
    "\n",
    "# Configure the root logger to INFO and include timestamps\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)  # ensure logging is enabled\n",
    "\n",
    "logging.info(\"About to call matmul() â€” inspect logs for execution flow\")\n",
    "\n",
    "d = nvmath.linalg.advanced.matmul(a, b, epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias})\n",
    "\n",
    "print(\"d type =\", type(d))\n",
    "print(\"d shape =\", d.shape)\n",
    "\n",
    "logging.info(\"Completed matmul() call\")\n",
    "logging.disable(logging.CRITICAL)  # disable logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7be58",
   "metadata": {},
   "source": [
    "As you can see, quite a bit of action has happened during the call to `matmul`:\n",
    "\n",
    "* **SPECIFICATION PHASE:** In this phase nvmath-python analyzes input arguments and creates an internal object `Matmul` with all required data prepared for the underlying cuBLASLt library call. Note, that this phase may take a noticeable overhead when inputs are relatively small.\n",
    "* **PLANNING PHASE:** This is where cuBLASLt analyzes prepared data from `Matmul` object and performs a search of a suitable algorithm to effectively perform an operation. It uses internal heuristics to select the most promising algorithm. The planning phase also introduces a noticeable overhead before actual computation even started.\n",
    "* **EXECUTION PHASE:** This is the phase where actual computation will happen.\n",
    "* **RESOURCE MANAGEMENT PHASE:** This the final phase where `Matmul` resource are released. Did you notice respective INFO line `The Matmul object's resources have been released` in the log?\n",
    "\n",
    "Now imaging, that our workflow assumes a **series of matrix multiplications**. In this scenario for each matrix multiplication we will go through all phases over and over again. What if matrix shapes and layouts do not change? What if input `dtypes` do not change? In this case it would be desirable to perform the *specification* and the *planning* once and amortize their cost through *multiple executions*. Exactly for this more sophisticated scenario nvmath-python is offering the **stateful** (or **class-form**) API for the matrix-matrix multiplication. In this API you rather construct an object (an instance of the class `Matmul` - did you notice the line in the logger telling `The Matmul operation has been created`?)\n",
    "\n",
    "Let us illustrate this with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "\n",
    "m, n, k, batch_size = 124, 10, 15, 8\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, dtype=cp.float32)\n",
    "\n",
    "\n",
    "def matmul_batched_stateless(a, b, bias):\n",
    "    for i in range(batch_size):\n",
    "        d[i] = nvmath.linalg.advanced.matmul(\n",
    "            a[i], b[i], epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[i]}\n",
    "        )\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def matmul_batched_stateful(a, b, bias):\n",
    "    with nvmath.linalg.advanced.Matmul(a[0], b[0]) as mm:\n",
    "        mm.plan(epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[0]})\n",
    "        mm.execute()\n",
    "        for i in range(1, batch_size):\n",
    "            mm.reset_operands(a=a[i], b=b[i], epilog_inputs={\"bias\": bias[i]})\n",
    "            d[i] = mm.execute()\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)\n",
    "\n",
    "logging.info(\"*************** Stateless API ***************\")\n",
    "\n",
    "matmul_batched_stateless(a, b, bias)\n",
    "\n",
    "logging.info(\"*************** Stateful API ***************\")\n",
    "\n",
    "matmul_batched_stateful(a, b, bias)\n",
    "\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d179437",
   "metadata": {},
   "source": [
    "You can see from the logger that in the case of stateless API there are 8 **SPECIFICATION**, 8 **PLANNING**, and 8 **EXECUTION** phases. In the case of stateful API implementation we managed to reduce the number of **SPECIFICATION** and **PLANNING** phases to 1. Let us see the performance impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ae4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    lambda: matmul_batched_stateful(a, b, bias), \"Stateful API\", lambda: matmul_batched_stateless(a, b, bias), \"Stateless API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6852f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Exercise 3. Batch dimension vs. batch sequence </h3>\n",
    "In the above example we implemented batching as a sequence of matrices being processed one by one in a loop. This is a common technique in streaming data case or when the entire batch does not fit into GPU memory. An alternative approach is to add a dedicated batching dimension and operate with the batch as a single tensor. The nvmath-python library supports both use cases.\n",
    "\n",
    "Implement a batching dimension approach and compare performance to a batch sequence approach. Explain performance difference (if any).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f9abe",
   "metadata": {},
   "source": [
    "## Autotuning with nvmath-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5d6d9",
   "metadata": {},
   "source": [
    "Using stateful APIs becomes even more essential when there is a need for autotuning. In many cases the built-in heuristics for `matmul` kernel selection work reasonably out-of-the-box. There are cases, however, where the underlying cuBLASLt library may choose suboptimal kernel and additional tuning is required. Let us see how such additional cost can be amortized through multiple executions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k, batch_size = 124, 1024, 1512, 1024\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, dtype=cp.float32)\n",
    "\n",
    "\n",
    "def matmul_batched_stateful_autotuned(a, b, bias):\n",
    "    with nvmath.linalg.advanced.Matmul(a[0], b[0]) as mm:\n",
    "        mm.plan(epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[0]})\n",
    "        mm.autotune(iterations=5)\n",
    "        mm.execute()\n",
    "        for i in range(1, batch_size):\n",
    "            mm.reset_operands(a=a[i], b=b[i], epilog_inputs={\"bias\": bias[i]})\n",
    "            d[i] = mm.execute()\n",
    "\n",
    "\n",
    "benchmark(\n",
    "    lambda: matmul_batched_stateful_autotuned(a, b, bias),\n",
    "    \"Stateful API with autotuning\",\n",
    "    lambda: matmul_batched_stateful(a, b, bias),\n",
    "    \"Stateful API without autotuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_stateful",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Takeaways</h3>\n",
    "\n",
    "- Stateless API is convenient for single operations but repeats specification and planning for each call.\n",
    "- Stateful API allows specification and planning once, then multiple executions, significantly improving performance for batched operations.\n",
    "- Four phases in nvmath-python operations: specification, planning, execution, and resource management.\n",
    "- Autotuning finds optimal kernels when built-in heuristics are suboptimal, providing additional performance gains.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
