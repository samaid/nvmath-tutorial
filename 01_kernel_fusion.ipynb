{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ca0c92",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#192015; color:#7fa637; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "\n",
    "![nvmath-python](_assets/nvmath_head_panel@0.25x.png)\n",
    "\n",
    "<p style=\"font-size:0.85em; margin-top:8px;\">\n",
    "Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES<br>\n",
    "SPDX-License-Identifier: BSD-3-Clause\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d2634",
   "metadata": {},
   "source": [
    "# Getting started with nvmath-python: kernel fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f205c2",
   "metadata": {},
   "source": [
    "In this tutorial we provide basic 101 about **nvmath-python**, how it fits in and plays with an existing scientific computing ecosystem in Python and what makes it a useful addition for this ecosystem. This notebook explains fundamentals of kernel fusion in nvmath-python\n",
    "\n",
    "<div class=\"alert alert-box alert-info\">\n",
    "    To use this notebook you will need a computer equipped with NVIDIA GPU as well as an environment with properly installed Python libraries and (optionally) CUDA Toolkit. Please refer to the nvmath-python documentation for getting familiar with <a href=\"https://docs.nvidia.com/cuda/nvmath-python/0.2.1/installation.html#install-nvmath-python\">installation options</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b36e2c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**nvmath-python** is powerful library designed to bridge the gap between Python's scientific computing community and **NVIDIA's CUDA-X math libraries**. The community has done a decent work in enabling GPU computing through **CuPy** and **PyTorch** and some other libraries and frameworks. They all leverage a goodness  of NVIDIA CUDA-X math libraries, such as cuBLAS and cuFFT, and demonstrate amazing performance gains compared to CPU libraries. At the same time, being constrained to *NumPy-like* APIs these GPU libraries do not always exploit full potential and advanced features of underlying NVIDIA CUDA-X libraries. This is where nvmath-python may become handy. It reimagined how math library API design can be intuitive, pythonic, and yet performant in sophisticated usage scenarios.\n",
    "\n",
    "Like other *NumPy-like* libraries, nvmath-python implements core numerical algorithms useful in many scientific and engineering fields. However, nvmath-python does not aim to replace or duplicate them. First and foremost, nvmath-python **is NOT** an *array library*, it does not implement traditional array library functionality, such as array *indexing* and *slicing*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4a6dd",
   "metadata": {},
   "source": [
    "## nvmath-python is NOT an array library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43072e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NumPy array creation, indexing and slicing\n",
    "import numpy as np\n",
    "\n",
    "# 1D array\n",
    "a = np.arange(10)  # create array with values from 0 to 9\n",
    "print(\"a =\", a)  # print the array\n",
    "print(\"a[2] =\", a[2])  # access the third element (index 2)\n",
    "print(\"a[2:7:2] =\", a[2:7:2])  # slice from index 2 to 6 with step 2\n",
    "\n",
    "# 2D array\n",
    "b = np.arange(12).reshape(3, 4)  # create 3x4 array (matrix)\n",
    "print(\"b =\", b)  # print the matrix\n",
    "print(\"b[0:2, 1:4] (submatrix) =\", b[0:2, 1:4])  # slice rows 0-1 and columns 1-3\n",
    "print(\"b[:,1] (second column) =\", b[:, 1])  # access all rows in second column\n",
    "print(\"b[0] (first row) =\", b[0])  # access first row\n",
    "print(\"b[1,2] =\", b[1, 2])  # access element at row 1, column 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e96dc8",
   "metadata": {},
   "source": [
    "Instead, nvmath-python is designed to **co-exist with array libraries** (NumPy, CuPy, PyTorch, *etc.*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nvmath\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "a_cpu = np.random.rand(n, k)\n",
    "b_cpu = np.random.rand(k, m)\n",
    "\n",
    "c_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)  # matrix multiplication\n",
    "print(\"c_cpu.shape =\", c_cpu.shape)  # should be (2, 4)\n",
    "print(type(c_cpu))  # should be numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd909ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvmath\n",
    "import cupy as cp\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "a_gpu = cp.random.rand(n, k)\n",
    "b_gpu = cp.random.rand(k, m)\n",
    "\n",
    "c_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)  # matrix multiplication\n",
    "print(\"c_gpu.shape =\", c_gpu.shape)  # should be (2, 4)\n",
    "print(type(c_gpu))  # should be cupy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc81dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nvmath\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "\n",
    "# CPU tensors\n",
    "a_cpu = torch.rand(n, k)\n",
    "b_cpu = torch.rand(k, m)\n",
    "\n",
    "# On CPU\n",
    "c_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "print(\"c_cpu.shape =\", c_cpu.shape)  # should be (2, 4)\n",
    "print(\"type(c_cpu) =\", type(c_cpu))  # should be torch.Tensor\n",
    "\n",
    "# Run on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    # move the same tensors to GPU\n",
    "    a_gpu = a_cpu.cuda()\n",
    "    b_gpu = b_cpu.cuda()\n",
    "    c_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "    print(\"c_gpu.shape =\", c_gpu.shape)\n",
    "    print(\"c_gpu device =\", getattr(c_gpu, \"device\", \"unknown\"))\n",
    "    print(\"type(c_gpu) =\", type(c_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927021c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Takeaways</h3>\n",
    "\n",
    "- nvmath-python accepts arrays from multiple libraries: NumPy (CPU), CuPy (GPU), and PyTorch (CPU/GPU).\n",
    "- To tell where a result lives use simple type/device checks — e.g. `isinstance(c, np.ndarray)`, `isinstance(c, cp.ndarray)` and `c.device`, or for PyTorch inspect `c.is_cuda` / `c.device`.\n",
    "- **Remember**: array semantics (indexing/slicing) are provided by the array library (NumPy/CuPy/PyTorch). nvmath-python focuses on math operations and interoperates with those array types.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b0df6",
   "metadata": {},
   "source": [
    "## Benchmarking GPU codes with `cupyx.profiler.benchmark`\n",
    "\n",
    "Since GPU kernels are launched asynchronously, a host call may return before the device work finishes. Naive timing (e.g. Python's `time.time()` method or Jupyter's `%%timeit`) measures only host-side overhead, not true device execution time. The `cupyx.profiler.benchmark` uses CUDA events, proper synchronization, warm-ups, and repeated runs to produce stable, device-level timing measurements. It removes much of the noise introduced by Python overhead, one-time setup costs, and asynchronous execution and reports aggregated statistics so you get reproducible, comparable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5762079",
   "metadata": {},
   "source": [
    "It's time to do real benchmarking to compare how nvmath-python performs on `matmul` relative to `cupy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174e1fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Practical notes</h3>\n",
    "\n",
    "- Make sure data is already allocated on device before benchmarking (transfer costs should be excluded unless you intend to measure them).\n",
    "- Run several repeats and inspect distributions (median is usually more robust than min or mean).\n",
    "- For end-to-end profiling (memory transfers, kernel launches, kernel internals) use NVIDIA tools like `nsys`, `nvprof`, or Nsight — `cupyx.profiler.benchmark` focuses on accurate kernel timing within Python workflows.\n",
    "- Watch out for GPU power/clock state and thermal throttling; for stable numbers, use consistent GPU governor/clock settings if available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k = 8192, 4096, 2048  # Use large enough sizes to get measurable times\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "\n",
    "# It's time to do real benchmarking to compare how nvmath-python\n",
    "# performs on `matmul` relative to `cupy`:\n",
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a, b),  # nvmath-python implementation\n",
    "    F_name=\"nvmath-python matmul\",\n",
    "    F_alternative=lambda: cp.matmul(a, b),  # CuPy implementation\n",
    "    F_alternative_name=\"CuPy matmul\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9a799",
   "metadata": {},
   "source": [
    "Is there anything wrong with nvmath-python? Why does \"advanced\" `matmul` run as good (or as bad?) as CuPy's `matmul`?\n",
    "\n",
    "The explanation is actually very simple: both CuPy and nvmath-python rely on the same cuBLAS library, they both perform nothing simpler than pure matrix-matrix multiplication, this is the reason (and the only reason) why we observe **identical** performance. To make a difference, we need to apply a little bit more **sophisticated use case** that does not simply translate to *NumPy-like* `matmul` call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77ce58",
   "metadata": {},
   "source": [
    "## Composite operations with nvmath-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a1ebf",
   "metadata": {},
   "source": [
    "In many real scenarios the `matmul` operation we considered earlier is chained with another operations. For example, in linear algebra GEMM, *Generalized Matrix Multiplication* is a commonly used building block in scientific and engineering applications.  In its simplified form GEMM performs the following operation:\n",
    "\n",
    "$$\n",
    "\\mathrm{D_{m \\times n} \\leftarrow \\alpha \\cdot (A_{m \\times k} \\cdot B_{k \\times n}) + \\beta \\cdot C_{m \\times n}}\n",
    "$$\n",
    "\n",
    "Using an array library one can easily implement this chained operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k = 10_000_000, 40, 10  # Take tall-and-skinny matrices to illustrate kernel fusion benefits\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "c = cp.random.rand(m, n, dtype=cp.float32)\n",
    "\n",
    "alpha = 1.5\n",
    "beta = 0.5\n",
    "\n",
    "d = alpha * a @ b + beta * c\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b6cc3",
   "metadata": {},
   "source": [
    "With nvmath-python you can also perform the composite operation, and you can do that with a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973c738",
   "metadata": {},
   "source": [
    "It's time to benchmark each alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta),  # nvmath-python implementation\n",
    "    F_name=\"nvmath-python GEMM\",\n",
    "    F_alternative=lambda: alpha * a @ b + beta * c,  # CuPy implementation\n",
    "    F_alternative_name=\"CuPy equivalent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4059f2",
   "metadata": {},
   "source": [
    "Both nvmath-python and CuPy perform the same composite operation, both are accelerated by the respective cuBLAS library. Why is nvmath-python significantly faster?\n",
    "\n",
    "The \"magic\" behind is called a kernel fusion. In the case of CuPy every basic operation is a separate function call on the host, which initiates a respective GPU kernel invocation asynchronously, returns to the host, submits the next kernel for execution, etc. In the case of nvmath-python the chained operation is performed as a whole in a single fused kernel. There are no accompanying overheads of multiple kernel invocations, but, more importantly, an execution as a fused kernel allows optimization of memory accesses, which significantly increases the *arithmetic intensity* of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0454e0e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#76B900; color:#ffffff; padding:12px; border-radius:8px; max-width:80%; width:auto; margin:0 auto;\">\n",
    "<h3 style=\"margin-top:0; color:#ffffff\">Exercise 1. Evaluate performance of CuPy <code>@</code>-based vs. <code>matmul</code>-based implementations of GEMM</h3>\n",
    "In the above examples we implemented GEMM using <code>@</code> operator notation for matrix multiplication. Implement CuPy variant using <code>matmul</code> function. Benchmark <code>@</code> variant vs. <code>matmul</code> variant and explain performance difference (if any).\n",
    "\n",
    "<strong>Hint</strong>: Consider operation precedence along with computational costs of each chained operation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a09170-9cfd-464e-9904-0aef36ad1002",
   "metadata": {},
   "source": [
    "## NVIDIA Nsight plugin for JupyterLab\n",
    "\n",
    "NVIDIA has created a useful JupyterLab plugin for the NVIDIA Nsight Tools, allowing to do a performance profiling from within notebooks. The following command will install the plugin into your environment.\n",
    "\n",
    "``` bash\n",
    "    pip install jupyterlab-nvidia-nsight\n",
    "```\n",
    "\n",
    "After the installation you will see a new tab **NVIDIA Nsight** in JupyterLab's menu. In the menu select **Profiling with Nsight Systems...**. This will restart the JupyterLab kernel. Select cells you wish to profile and execute **Run and profile selected cells...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef748d-aa10-41be-a106-bac30acdd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile this cell with Nsight Systems...\n",
    "d = alpha * a @ b + beta * c\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb88ca-12ac-4bf3-9b9a-48119bbe89e4",
   "metadata": {},
   "source": [
    "After running the profiler and opening the generated report you will be able to see something like this\n",
    "\n",
    "<img src=\"./_assets/nsys-report-cupy.png\" alt=\"Nsight Systems report example\" width=\"75%\"/>\n",
    "\n",
    "Note the number of kernels and their execution times. You should see that CuPy multiply kernel takes longest and consumes more time than SGEMM kernel.\n",
    "\n",
    "Let us profile nvmath-python's implementation now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile this cell with Nsight Systems...\n",
    "d = nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta)\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13450b",
   "metadata": {},
   "source": [
    "By opening the **Nsight Systems** UI you should be able to see something like this\n",
    "\n",
    "<img src=\"./_assets/nsys-report-nvmath.png\" alt=\"Nsight Systems report example\" width=\"75%\"/>\n",
    "\n",
    "Note the only kernel in the timeline is SGEMM. Everything else has been fused into that kernel. This is the true reason why nvmath-python's performance is much better in a composite operation like GEMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1edb4b",
   "metadata": {},
   "source": [
    "## GEMM with fused epilogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d60caf",
   "metadata": {},
   "source": [
    " The library goes one step further and allows fusion of GEMM operation with an epilog function $f(x)$, which is applied element-wise to the result of GEMM operation. Specifically,\n",
    "\n",
    "$$\n",
    "\\mathrm{D_{m \\times n}} \\leftarrow f(\\mathrm{\\alpha \\cdot (A_{m \\times k} \\cdot B_{k \\times n}) + \\beta \\cdot C_{m \\times n}})\n",
    "$$\n",
    "\n",
    "For deeper dive into GEMM epologs and other advanced techniqus of the matrix-matrix multiplication in the nvmath-python, please refer to a collection of notebooks under `notebooks/matmul/` directory:\n",
    "* [notebooks/matmul/01_introduction.ipynb](../matmul/01_introduction.ipynb)\n",
    "* [notebooks/matmul/02_epilogs.ipynb](../matmul/02_epilogs.ipynb)\n",
    "* [notebooks/matmul/03_backpropagation.ipynb](../matmul/03_backpropagation.ipynb)\n",
    "* [notebooks/matmul/04_fp8.ipynb](../matmul/04_fp8.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
